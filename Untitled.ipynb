{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-566719c92584>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfuncs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "from model import *\n",
    "from funcs import *\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "GAME = \"BreakoutDeterministic-v4\" # \"BreakoutDeterministic-v0\"\n",
    "\n",
    "# Atari Breakout actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) \n",
    "VALID_ACTIONS = [0, 1, 2, 3]\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "# set parameters for running\n",
    "\n",
    "train_or_test = 'train' #'test' #'train'\n",
    "train_from_scratch = True\n",
    "start_iter = 0\n",
    "start_episode = 0\n",
    "epsilon_start = 1.0\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "env = gym.envs.make(GAME)\n",
    "\n",
    "print(\"Action space size: {}\".format(env.action_space.n))\n",
    "observation = env.reset()\n",
    "print(\"Observation space shape: {}\".format(observation.shape))\n",
    "\n",
    "plt.figure()\n",
    "plt.imsave(\"Atari_Breakout1.png\", env.render(mode='rgb_array'))\n",
    "\n",
    "env.step(2)\n",
    "env.step(1)\n",
    "\n",
    "plt.figure()\n",
    "plt.imsave(\"Atari_Breakout2.png\", env.render(mode='rgb_array'))\n",
    "env.close() \n",
    "\n",
    "env.step(2)\n",
    "env.step(0)\n",
    "\n",
    "plt.figure()\n",
    "plt.imsave(\"Atari_Breakout3.png\", env.render(mode='rgb_array'))\n",
    "env.close() \n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# experiment dir\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "\n",
    "# create ckpt directory    \n",
    "checkpoint_dir = os.path.join(experiment_dir, \"ckpt\")\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    \n",
    "if not os.path.exists(checkpoint_dir):\n",
    "   os.makedirs(checkpoint_dir)\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def deep_q_learning(sess, env, q_net, target_net, state_processor, num_episodes, train_or_test='train', train_from_scratch=True,\n",
    "                    start_iter=0, start_episode=0, replay_memory_size=250000, replay_memory_init_size=50000, update_target_net_every=10000,\n",
    "                    gamma=0.99, epsilon_start=1.0, epsilon_end=[0.1,0.01], epsilon_decay_steps=[1e6,1e6], batch_size=32):\n",
    "                   \n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # policy \n",
    "    policy = epsilon_greedy_policy(q_net, len(VALID_ACTIONS))\n",
    "\n",
    "\n",
    "    # populate replay memory\n",
    "    if (train_or_test == 'train'):\n",
    "      print(\"populating replay memory\")\n",
    "      replay_memory = populate_replay_mem(sess, env, state_processor, replay_memory_init_size, policy, epsilon_start, \n",
    "                                                       epsilon_end[0], epsilon_decay_steps[0], VALID_ACTIONS, Transition)\n",
    "\n",
    "\n",
    "    # epsilon start\n",
    "    if (train_or_test == 'train'):\n",
    "       delta_epsilon1 = (epsilon_start - epsilon_end[0])/float(epsilon_decay_steps[0])     \n",
    "       delta_epsilon2 = (epsilon_end[0] - epsilon_end[1])/float(epsilon_decay_steps[1])    \n",
    "       if (train_from_scratch == True):\n",
    "          epsilon = epsilon_start\n",
    "       else:\n",
    "          if (start_iter <= epsilon_decay_steps[0]):\n",
    "             epsilon = max(epsilon_start - float(start_iter) * delta_epsilon1, epsilon_end[0])\n",
    "          elif (start_iter > epsilon_decay_steps[0] and start_iter < epsilon_decay_steps[0]+epsilon_decay_steps[1]):\n",
    "             epsilon = max(epsilon_end[0] - float(start_iter) * delta_epsilon2, epsilon_end[1])\n",
    "          else:\n",
    "             epsilon = epsilon_end[1]      \n",
    "    elif (train_or_test == 'test'):\n",
    "       epsilon = epsilon_end[1]\n",
    "\n",
    "\n",
    "    # total number of time steps \n",
    "    total_t = start_iter\n",
    "\n",
    "\n",
    "    for ep in range(start_episode, num_episodes):\n",
    "\n",
    "        # save ckpt\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # env reset\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "\n",
    "        loss = 0.0\n",
    "        time_steps = 0\n",
    "        episode_rewards = 0.0\n",
    "    \n",
    "        ale_lives = 5\n",
    "        info_ale_lives = ale_lives\n",
    "        steps_in_this_life = 1000000\n",
    "        num_no_ops_this_life = 0\n",
    "\n",
    "\n",
    "\n",
    "        while True:\n",
    "            \n",
    "            if (train_or_test == 'train'):\n",
    "              #epsilon = max(epsilon - delta_epsilon, epsilon_end) \n",
    "              if (total_t <= epsilon_decay_steps[0]):\n",
    "                    epsilon = max(epsilon - delta_epsilon1, epsilon_end[0]) \n",
    "              elif (total_t >= epsilon_decay_steps[0] and total_t <= epsilon_decay_steps[0]+epsilon_decay_steps[1]):\n",
    "                    epsilon = epsilon_end[0] - (epsilon_end[0]-epsilon_end[1]) / float(epsilon_decay_steps[1]) * float(total_t-epsilon_decay_steps[0]) \n",
    "                    epsilon = max(epsilon, epsilon_end[1])           \n",
    "              else:\n",
    "                    epsilon = epsilon_end[1]\n",
    "\n",
    "\n",
    "              # update target net\n",
    "              if total_t % update_target_net_every == 0:\n",
    "                 copy_model_parameters(sess, q_net, target_net)\n",
    "                 print(\"\\n copied params from Q net to target net \")\n",
    "\n",
    "                   \n",
    "            time_to_fire = False\n",
    "            if (time_steps == 0 or ale_lives != info_ale_lives):\n",
    "               # new game or new life \n",
    "               steps_in_this_life = 0\n",
    "               num_no_ops_this_life = np.random.randint(low=0,high=7)\n",
    "               action_probs = [0.0, 1.0, 0.0, 0.0]  # fire\n",
    "               time_to_fire = True\n",
    "               if (ale_lives != info_ale_lives):\n",
    "                  ale_lives = info_ale_lives\n",
    "            else:\n",
    "               action_probs = policy(sess, state, epsilon)\n",
    "\n",
    "            steps_in_this_life += 1 \n",
    "            if (steps_in_this_life < num_no_ops_this_life and not time_to_fire):\n",
    "               # no-op\n",
    "               action_probs = [1.0, 0.0, 0.0, 0.0] # no-op\n",
    "\n",
    "\n",
    "\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "             \n",
    "            env.render()\n",
    "            next_state_img, reward, done, info = env.step(VALID_ACTIONS[action]) \n",
    "            \n",
    "            info_ale_lives = int(info['ale.lives'])\n",
    "\n",
    "            # rewards = -1,0,+1 as done in the paper\n",
    "            #reward = np.sign(reward)\n",
    "\n",
    "            next_state_img = state_processor.process(sess, next_state_img)\n",
    "\n",
    "            # state is of size [84,84,4]; next_state_img is of size[84,84]\n",
    "            #next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "            next_state = np.zeros((84,84,4),dtype=np.uint8)\n",
    "            next_state[:,:,0] = state[:,:,1] \n",
    "            next_state[:,:,1] = state[:,:,2]\n",
    "            next_state[:,:,2] = state[:,:,3]\n",
    "            next_state[:,:,3] = next_state_img    \n",
    "\n",
    "\n",
    "            episode_rewards += reward  \n",
    "            time_steps += 1\n",
    "\n",
    " \n",
    "            if (train_or_test == 'train'):\n",
    "\n",
    "                # if replay memory is full, pop the first element\n",
    "                if len(replay_memory) == replay_memory_size:\n",
    "                    replay_memory.pop(0)\n",
    "\n",
    "                # save transition to replay memory\n",
    "                # done = True in replay memory for every loss of life \n",
    "                if (ale_lives == info_ale_lives):\n",
    "                   replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "                else:\n",
    "                   #print('loss of life ')\n",
    "                   replay_memory.append(Transition(state, action, reward, next_state, True))               \n",
    "\n",
    "                # sample a minibatch from replay memory\n",
    "                samples = random.sample(replay_memory, batch_size)\n",
    "                states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "\n",
    "                # calculate q values and targets \n",
    "                q_values_next = target_net.predict(sess, next_states_batch)\n",
    "                greedy_q = np.amax(q_values_next, axis=1) \n",
    "                targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * gamma * greedy_q\n",
    "               \n",
    "\n",
    "                # update net \n",
    "                if (total_t % 4 == 0):\n",
    "                   states_batch = np.array(states_batch)\n",
    "                   loss = q_net.update(sess, states_batch, action_batch, targets_batch)\n",
    "\n",
    "            if done:\n",
    "                #print(\"done: \", done)\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "            \n",
    "\n",
    "        if (train_or_test == 'train'): \n",
    "           print('\\n Eisode: ', ep, '| time steps: ', time_steps, '| total episode reward: ', episode_rewards, '| total_t: ', total_t, '| epsilon: ', epsilon, '| replay mem size: ', len(replay_memory))\n",
    "        elif (train_or_test == 'test'):\n",
    "           print('\\n Eisode: ', ep, '| time steps: ', time_steps, '| total episode reward: ', episode_rewards, '| total_t: ', total_t, '| epsilon: ', epsilon)\n",
    "\n",
    "\n",
    "        if (train_or_test == 'train'):\n",
    "            f = open(\"experiments/\" + str(env.spec.id) + \"/performance.txt\", \"a+\")\n",
    "            f.write(str(ep) + \" \" + str(time_steps) + \" \" + str(episode_rewards) + \" \" + str(total_t) + \" \" + str(epsilon) + '\\n')  \n",
    "            f.close()\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# Q and target networks \n",
    "q_net = QNetwork(scope=\"q\",VALID_ACTIONS=VALID_ACTIONS)\n",
    "target_net = QNetwork(scope=\"target_q\", VALID_ACTIONS=VALID_ACTIONS)\n",
    "\n",
    "# state processor\n",
    "state_processor = ImageProcess()\n",
    "\n",
    "# tf saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    " \n",
    "      # load model/ initialize model\n",
    "      if ((train_or_test == 'train' and train_from_scratch == False) or train_or_test == 'test'):\n",
    "                 latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "                 print(\"loading model ckpt {}...\\n\".format(latest_checkpoint))\n",
    "                 saver.restore(sess, latest_checkpoint)\n",
    "      elif (train_or_test == 'train' and train_from_scratch == True):\n",
    "                 sess.run(tf.global_variables_initializer())    \n",
    "\n",
    "\n",
    "\n",
    "      # run\n",
    "      deep_q_learning(sess, env, q_net=q_net, target_net=target_net, state_processor=state_processor, num_episodes=25000,\n",
    "                            train_or_test=train_or_test, train_from_scratch=train_from_scratch, start_iter=start_iter, start_episode=start_episode,\n",
    "                                    replay_memory_size=300000, replay_memory_init_size=5000, update_target_net_every=10000,\n",
    "                                    gamma=0.99, epsilon_start=epsilon_start, epsilon_end=[0.1,0.01], epsilon_decay_steps=[1e6,1e6], batch_size=32)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
